app = "supakavadee-r-llm"
primary_region = "dfw"

[build]
  image = "ghcr.io/ggerganov/llama.cpp:server"

[env]
  PORT = "11434"

[mounts]
  source = "supakavadee_models"
  destination = "/models"

[http_services]
  internal_port = 11434
  force_https = true
  auto_stop_machines = true
  auto_start_machines = true
  min_machines_running = 0
  processes = ["app"]

  [[http_services.machine]]
    cpu_kind = "shared"
    cpus = 2
    memory_mb = 1024

[machines]
  cpu_kind = "shared"
  cpus = 2
  memory_mb = 1024