{
  "best_metric": 0.6719242902208202,
  "best_model_checkpoint": "./results/checkpoint-713",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 2139,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.014025245441795231,
      "grad_norm": 1.2492282390594482,
      "learning_rate": 1.9906498363721366e-05,
      "loss": 0.7301,
      "step": 10
    },
    {
      "epoch": 0.028050490883590462,
      "grad_norm": 2.2334651947021484,
      "learning_rate": 1.981299672744273e-05,
      "loss": 0.7167,
      "step": 20
    },
    {
      "epoch": 0.04207573632538569,
      "grad_norm": 2.9322457313537598,
      "learning_rate": 1.9719495091164096e-05,
      "loss": 0.6968,
      "step": 30
    },
    {
      "epoch": 0.056100981767180924,
      "grad_norm": 2.2000913619995117,
      "learning_rate": 1.962599345488546e-05,
      "loss": 0.6945,
      "step": 40
    },
    {
      "epoch": 0.07012622720897616,
      "grad_norm": 1.7338263988494873,
      "learning_rate": 1.953249181860683e-05,
      "loss": 0.6904,
      "step": 50
    },
    {
      "epoch": 0.08415147265077139,
      "grad_norm": 1.6980326175689697,
      "learning_rate": 1.9438990182328193e-05,
      "loss": 0.7046,
      "step": 60
    },
    {
      "epoch": 0.09817671809256662,
      "grad_norm": 5.99036169052124,
      "learning_rate": 1.9345488546049558e-05,
      "loss": 0.6667,
      "step": 70
    },
    {
      "epoch": 0.11220196353436185,
      "grad_norm": 1.2999191284179688,
      "learning_rate": 1.9251986909770922e-05,
      "loss": 0.6856,
      "step": 80
    },
    {
      "epoch": 0.12622720897615708,
      "grad_norm": 1.3758997917175293,
      "learning_rate": 1.9158485273492287e-05,
      "loss": 0.6789,
      "step": 90
    },
    {
      "epoch": 0.1402524544179523,
      "grad_norm": 2.421232223510742,
      "learning_rate": 1.906498363721365e-05,
      "loss": 0.657,
      "step": 100
    },
    {
      "epoch": 0.15427769985974754,
      "grad_norm": 1.272135615348816,
      "learning_rate": 1.8971482000935016e-05,
      "loss": 0.7031,
      "step": 110
    },
    {
      "epoch": 0.16830294530154277,
      "grad_norm": 1.4296773672103882,
      "learning_rate": 1.8877980364656384e-05,
      "loss": 0.6577,
      "step": 120
    },
    {
      "epoch": 0.182328190743338,
      "grad_norm": 1.9152387380599976,
      "learning_rate": 1.878447872837775e-05,
      "loss": 0.6625,
      "step": 130
    },
    {
      "epoch": 0.19635343618513323,
      "grad_norm": 1.1623197793960571,
      "learning_rate": 1.8690977092099114e-05,
      "loss": 0.7012,
      "step": 140
    },
    {
      "epoch": 0.21037868162692847,
      "grad_norm": 1.0698353052139282,
      "learning_rate": 1.8597475455820478e-05,
      "loss": 0.6655,
      "step": 150
    },
    {
      "epoch": 0.2244039270687237,
      "grad_norm": 3.6718904972076416,
      "learning_rate": 1.8503973819541843e-05,
      "loss": 0.6612,
      "step": 160
    },
    {
      "epoch": 0.23842917251051893,
      "grad_norm": 2.055206298828125,
      "learning_rate": 1.8410472183263208e-05,
      "loss": 0.6724,
      "step": 170
    },
    {
      "epoch": 0.25245441795231416,
      "grad_norm": 1.1888670921325684,
      "learning_rate": 1.8316970546984572e-05,
      "loss": 0.6591,
      "step": 180
    },
    {
      "epoch": 0.2664796633941094,
      "grad_norm": 1.7433427572250366,
      "learning_rate": 1.822346891070594e-05,
      "loss": 0.643,
      "step": 190
    },
    {
      "epoch": 0.2805049088359046,
      "grad_norm": 0.7679880857467651,
      "learning_rate": 1.8129967274427305e-05,
      "loss": 0.6517,
      "step": 200
    },
    {
      "epoch": 0.29453015427769985,
      "grad_norm": 2.099923849105835,
      "learning_rate": 1.803646563814867e-05,
      "loss": 0.617,
      "step": 210
    },
    {
      "epoch": 0.3085553997194951,
      "grad_norm": 1.258429765701294,
      "learning_rate": 1.7942964001870034e-05,
      "loss": 0.669,
      "step": 220
    },
    {
      "epoch": 0.3225806451612903,
      "grad_norm": 2.677124500274658,
      "learning_rate": 1.78494623655914e-05,
      "loss": 0.6455,
      "step": 230
    },
    {
      "epoch": 0.33660589060308554,
      "grad_norm": 1.4889492988586426,
      "learning_rate": 1.7755960729312764e-05,
      "loss": 0.6407,
      "step": 240
    },
    {
      "epoch": 0.3506311360448808,
      "grad_norm": 1.2614237070083618,
      "learning_rate": 1.766245909303413e-05,
      "loss": 0.6949,
      "step": 250
    },
    {
      "epoch": 0.364656381486676,
      "grad_norm": 0.7463633418083191,
      "learning_rate": 1.7568957456755496e-05,
      "loss": 0.6775,
      "step": 260
    },
    {
      "epoch": 0.37868162692847124,
      "grad_norm": 4.910980701446533,
      "learning_rate": 1.7475455820476858e-05,
      "loss": 0.6834,
      "step": 270
    },
    {
      "epoch": 0.39270687237026647,
      "grad_norm": 4.634937286376953,
      "learning_rate": 1.7381954184198226e-05,
      "loss": 0.6045,
      "step": 280
    },
    {
      "epoch": 0.4067321178120617,
      "grad_norm": 1.383968710899353,
      "learning_rate": 1.728845254791959e-05,
      "loss": 0.6333,
      "step": 290
    },
    {
      "epoch": 0.42075736325385693,
      "grad_norm": 1.3362953662872314,
      "learning_rate": 1.7194950911640955e-05,
      "loss": 0.693,
      "step": 300
    },
    {
      "epoch": 0.43478260869565216,
      "grad_norm": 0.9905798435211182,
      "learning_rate": 1.710144927536232e-05,
      "loss": 0.6906,
      "step": 310
    },
    {
      "epoch": 0.4488078541374474,
      "grad_norm": 1.3900221586227417,
      "learning_rate": 1.7007947639083688e-05,
      "loss": 0.6548,
      "step": 320
    },
    {
      "epoch": 0.4628330995792426,
      "grad_norm": 1.1005585193634033,
      "learning_rate": 1.691444600280505e-05,
      "loss": 0.6947,
      "step": 330
    },
    {
      "epoch": 0.47685834502103785,
      "grad_norm": 2.08528208732605,
      "learning_rate": 1.6820944366526414e-05,
      "loss": 0.6831,
      "step": 340
    },
    {
      "epoch": 0.4908835904628331,
      "grad_norm": 1.756514072418213,
      "learning_rate": 1.672744273024778e-05,
      "loss": 0.6483,
      "step": 350
    },
    {
      "epoch": 0.5049088359046283,
      "grad_norm": 1.3684967756271362,
      "learning_rate": 1.6633941093969146e-05,
      "loss": 0.6566,
      "step": 360
    },
    {
      "epoch": 0.5189340813464236,
      "grad_norm": 1.0169973373413086,
      "learning_rate": 1.654043945769051e-05,
      "loss": 0.6275,
      "step": 370
    },
    {
      "epoch": 0.5329593267882188,
      "grad_norm": 1.5496740341186523,
      "learning_rate": 1.6446937821411876e-05,
      "loss": 0.6485,
      "step": 380
    },
    {
      "epoch": 0.5469845722300141,
      "grad_norm": 1.9520330429077148,
      "learning_rate": 1.635343618513324e-05,
      "loss": 0.6956,
      "step": 390
    },
    {
      "epoch": 0.5610098176718092,
      "grad_norm": 1.5149405002593994,
      "learning_rate": 1.6259934548854605e-05,
      "loss": 0.6604,
      "step": 400
    },
    {
      "epoch": 0.5750350631136045,
      "grad_norm": 0.9887269735336304,
      "learning_rate": 1.616643291257597e-05,
      "loss": 0.6439,
      "step": 410
    },
    {
      "epoch": 0.5890603085553997,
      "grad_norm": 1.8783255815505981,
      "learning_rate": 1.6072931276297338e-05,
      "loss": 0.608,
      "step": 420
    },
    {
      "epoch": 0.603085553997195,
      "grad_norm": 4.176484107971191,
      "learning_rate": 1.5979429640018702e-05,
      "loss": 0.6821,
      "step": 430
    },
    {
      "epoch": 0.6171107994389902,
      "grad_norm": 1.957058310508728,
      "learning_rate": 1.5885928003740067e-05,
      "loss": 0.6783,
      "step": 440
    },
    {
      "epoch": 0.6311360448807855,
      "grad_norm": 1.1765317916870117,
      "learning_rate": 1.579242636746143e-05,
      "loss": 0.6382,
      "step": 450
    },
    {
      "epoch": 0.6451612903225806,
      "grad_norm": 3.4721055030822754,
      "learning_rate": 1.5698924731182796e-05,
      "loss": 0.6188,
      "step": 460
    },
    {
      "epoch": 0.6591865357643759,
      "grad_norm": 1.7478383779525757,
      "learning_rate": 1.560542309490416e-05,
      "loss": 0.6317,
      "step": 470
    },
    {
      "epoch": 0.6732117812061711,
      "grad_norm": 1.382447600364685,
      "learning_rate": 1.551192145862553e-05,
      "loss": 0.5571,
      "step": 480
    },
    {
      "epoch": 0.6872370266479664,
      "grad_norm": 1.4404714107513428,
      "learning_rate": 1.5418419822346894e-05,
      "loss": 0.6624,
      "step": 490
    },
    {
      "epoch": 0.7012622720897616,
      "grad_norm": 1.9600410461425781,
      "learning_rate": 1.5324918186068258e-05,
      "loss": 0.6389,
      "step": 500
    },
    {
      "epoch": 0.7152875175315568,
      "grad_norm": 1.813097596168518,
      "learning_rate": 1.5231416549789621e-05,
      "loss": 0.7069,
      "step": 510
    },
    {
      "epoch": 0.729312762973352,
      "grad_norm": 3.1740875244140625,
      "learning_rate": 1.5137914913510988e-05,
      "loss": 0.6686,
      "step": 520
    },
    {
      "epoch": 0.7433380084151473,
      "grad_norm": 2.959303855895996,
      "learning_rate": 1.5044413277232352e-05,
      "loss": 0.5649,
      "step": 530
    },
    {
      "epoch": 0.7573632538569425,
      "grad_norm": 1.212415099143982,
      "learning_rate": 1.4950911640953719e-05,
      "loss": 0.5438,
      "step": 540
    },
    {
      "epoch": 0.7713884992987378,
      "grad_norm": 1.5546098947525024,
      "learning_rate": 1.4857410004675083e-05,
      "loss": 0.6247,
      "step": 550
    },
    {
      "epoch": 0.7854137447405329,
      "grad_norm": 6.022912979125977,
      "learning_rate": 1.476390836839645e-05,
      "loss": 0.6123,
      "step": 560
    },
    {
      "epoch": 0.7994389901823282,
      "grad_norm": 4.357657432556152,
      "learning_rate": 1.4670406732117813e-05,
      "loss": 0.5558,
      "step": 570
    },
    {
      "epoch": 0.8134642356241234,
      "grad_norm": 5.223540306091309,
      "learning_rate": 1.4576905095839177e-05,
      "loss": 0.5699,
      "step": 580
    },
    {
      "epoch": 0.8274894810659187,
      "grad_norm": 2.5577306747436523,
      "learning_rate": 1.4483403459560544e-05,
      "loss": 0.6334,
      "step": 590
    },
    {
      "epoch": 0.8415147265077139,
      "grad_norm": 4.821260452270508,
      "learning_rate": 1.4389901823281908e-05,
      "loss": 0.6853,
      "step": 600
    },
    {
      "epoch": 0.8555399719495091,
      "grad_norm": 1.4710830450057983,
      "learning_rate": 1.4296400187003275e-05,
      "loss": 0.5195,
      "step": 610
    },
    {
      "epoch": 0.8695652173913043,
      "grad_norm": 4.773658752441406,
      "learning_rate": 1.420289855072464e-05,
      "loss": 0.595,
      "step": 620
    },
    {
      "epoch": 0.8835904628330996,
      "grad_norm": 2.037386894226074,
      "learning_rate": 1.4109396914446002e-05,
      "loss": 0.5585,
      "step": 630
    },
    {
      "epoch": 0.8976157082748948,
      "grad_norm": 1.46725594997406,
      "learning_rate": 1.4015895278167369e-05,
      "loss": 0.6413,
      "step": 640
    },
    {
      "epoch": 0.9116409537166901,
      "grad_norm": 2.9806041717529297,
      "learning_rate": 1.3922393641888733e-05,
      "loss": 0.5991,
      "step": 650
    },
    {
      "epoch": 0.9256661991584852,
      "grad_norm": 3.4823453426361084,
      "learning_rate": 1.38288920056101e-05,
      "loss": 0.658,
      "step": 660
    },
    {
      "epoch": 0.9396914446002805,
      "grad_norm": 7.752590179443359,
      "learning_rate": 1.3735390369331464e-05,
      "loss": 0.6888,
      "step": 670
    },
    {
      "epoch": 0.9537166900420757,
      "grad_norm": 4.936214923858643,
      "learning_rate": 1.364188873305283e-05,
      "loss": 0.6182,
      "step": 680
    },
    {
      "epoch": 0.967741935483871,
      "grad_norm": 2.305534601211548,
      "learning_rate": 1.3548387096774194e-05,
      "loss": 0.664,
      "step": 690
    },
    {
      "epoch": 0.9817671809256662,
      "grad_norm": 1.93379545211792,
      "learning_rate": 1.345488546049556e-05,
      "loss": 0.6386,
      "step": 700
    },
    {
      "epoch": 0.9957924263674615,
      "grad_norm": 3.4416818618774414,
      "learning_rate": 1.3361383824216925e-05,
      "loss": 0.6675,
      "step": 710
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.6719242902208202,
      "eval_f1": 0.632176710749743,
      "eval_loss": 0.6332257986068726,
      "eval_precision": 0.674665500962137,
      "eval_recall": 0.6719242902208202,
      "eval_runtime": 24.3378,
      "eval_samples_per_second": 26.05,
      "eval_steps_per_second": 3.287,
      "step": 713
    },
    {
      "epoch": 1.0098176718092566,
      "grad_norm": 1.172363519668579,
      "learning_rate": 1.3267882187938291e-05,
      "loss": 0.7045,
      "step": 720
    },
    {
      "epoch": 1.023842917251052,
      "grad_norm": 4.345941066741943,
      "learning_rate": 1.3174380551659656e-05,
      "loss": 0.6981,
      "step": 730
    },
    {
      "epoch": 1.0378681626928472,
      "grad_norm": 2.186760663986206,
      "learning_rate": 1.3080878915381022e-05,
      "loss": 0.6198,
      "step": 740
    },
    {
      "epoch": 1.0518934081346423,
      "grad_norm": 4.041953086853027,
      "learning_rate": 1.2987377279102385e-05,
      "loss": 0.6372,
      "step": 750
    },
    {
      "epoch": 1.0659186535764376,
      "grad_norm": 5.182562828063965,
      "learning_rate": 1.289387564282375e-05,
      "loss": 0.6925,
      "step": 760
    },
    {
      "epoch": 1.0799438990182328,
      "grad_norm": 3.350630283355713,
      "learning_rate": 1.2800374006545116e-05,
      "loss": 0.6021,
      "step": 770
    },
    {
      "epoch": 1.0939691444600281,
      "grad_norm": 2.7669854164123535,
      "learning_rate": 1.270687237026648e-05,
      "loss": 0.6793,
      "step": 780
    },
    {
      "epoch": 1.1079943899018232,
      "grad_norm": 1.5442107915878296,
      "learning_rate": 1.2613370733987847e-05,
      "loss": 0.6159,
      "step": 790
    },
    {
      "epoch": 1.1220196353436185,
      "grad_norm": 3.080660581588745,
      "learning_rate": 1.2519869097709212e-05,
      "loss": 0.5813,
      "step": 800
    },
    {
      "epoch": 1.1360448807854138,
      "grad_norm": 2.4578967094421387,
      "learning_rate": 1.2426367461430575e-05,
      "loss": 0.6429,
      "step": 810
    },
    {
      "epoch": 1.150070126227209,
      "grad_norm": 5.287307262420654,
      "learning_rate": 1.2332865825151941e-05,
      "loss": 0.6441,
      "step": 820
    },
    {
      "epoch": 1.1640953716690041,
      "grad_norm": 2.35724139213562,
      "learning_rate": 1.2239364188873306e-05,
      "loss": 0.6746,
      "step": 830
    },
    {
      "epoch": 1.1781206171107994,
      "grad_norm": 7.8835368156433105,
      "learning_rate": 1.2145862552594672e-05,
      "loss": 0.6666,
      "step": 840
    },
    {
      "epoch": 1.1921458625525947,
      "grad_norm": 6.889536380767822,
      "learning_rate": 1.2052360916316037e-05,
      "loss": 0.6175,
      "step": 850
    },
    {
      "epoch": 1.20617110799439,
      "grad_norm": 2.1502151489257812,
      "learning_rate": 1.1958859280037403e-05,
      "loss": 0.6833,
      "step": 860
    },
    {
      "epoch": 1.220196353436185,
      "grad_norm": 2.367459535598755,
      "learning_rate": 1.1865357643758766e-05,
      "loss": 0.634,
      "step": 870
    },
    {
      "epoch": 1.2342215988779803,
      "grad_norm": 1.6201369762420654,
      "learning_rate": 1.177185600748013e-05,
      "loss": 0.6985,
      "step": 880
    },
    {
      "epoch": 1.2482468443197756,
      "grad_norm": 2.847860336303711,
      "learning_rate": 1.1678354371201497e-05,
      "loss": 0.7186,
      "step": 890
    },
    {
      "epoch": 1.262272089761571,
      "grad_norm": 1.0708945989608765,
      "learning_rate": 1.1584852734922862e-05,
      "loss": 0.6113,
      "step": 900
    },
    {
      "epoch": 1.276297335203366,
      "grad_norm": 2.6524205207824707,
      "learning_rate": 1.1491351098644228e-05,
      "loss": 0.7154,
      "step": 910
    },
    {
      "epoch": 1.2903225806451613,
      "grad_norm": 5.109647274017334,
      "learning_rate": 1.1397849462365593e-05,
      "loss": 0.5918,
      "step": 920
    },
    {
      "epoch": 1.3043478260869565,
      "grad_norm": 1.4854975938796997,
      "learning_rate": 1.1304347826086957e-05,
      "loss": 0.6097,
      "step": 930
    },
    {
      "epoch": 1.3183730715287518,
      "grad_norm": 3.4647762775421143,
      "learning_rate": 1.1210846189808322e-05,
      "loss": 0.5971,
      "step": 940
    },
    {
      "epoch": 1.3323983169705471,
      "grad_norm": 1.924927830696106,
      "learning_rate": 1.1117344553529688e-05,
      "loss": 0.6508,
      "step": 950
    },
    {
      "epoch": 1.3464235624123422,
      "grad_norm": 3.2923238277435303,
      "learning_rate": 1.1023842917251053e-05,
      "loss": 0.6205,
      "step": 960
    },
    {
      "epoch": 1.3604488078541375,
      "grad_norm": 5.103480815887451,
      "learning_rate": 1.093034128097242e-05,
      "loss": 0.6088,
      "step": 970
    },
    {
      "epoch": 1.3744740532959328,
      "grad_norm": 6.126786708831787,
      "learning_rate": 1.0836839644693784e-05,
      "loss": 0.5889,
      "step": 980
    },
    {
      "epoch": 1.3884992987377278,
      "grad_norm": 1.8885860443115234,
      "learning_rate": 1.0743338008415147e-05,
      "loss": 0.6767,
      "step": 990
    },
    {
      "epoch": 1.402524544179523,
      "grad_norm": 5.755500793457031,
      "learning_rate": 1.0649836372136513e-05,
      "loss": 0.6105,
      "step": 1000
    },
    {
      "epoch": 1.4165497896213184,
      "grad_norm": 1.678700566291809,
      "learning_rate": 1.0556334735857878e-05,
      "loss": 0.6024,
      "step": 1010
    },
    {
      "epoch": 1.4305750350631137,
      "grad_norm": 1.6279953718185425,
      "learning_rate": 1.0462833099579244e-05,
      "loss": 0.5543,
      "step": 1020
    },
    {
      "epoch": 1.444600280504909,
      "grad_norm": 3.0552968978881836,
      "learning_rate": 1.0369331463300609e-05,
      "loss": 0.5991,
      "step": 1030
    },
    {
      "epoch": 1.458625525946704,
      "grad_norm": 2.1405742168426514,
      "learning_rate": 1.0275829827021975e-05,
      "loss": 0.6434,
      "step": 1040
    },
    {
      "epoch": 1.4726507713884993,
      "grad_norm": 5.302210330963135,
      "learning_rate": 1.0182328190743338e-05,
      "loss": 0.6584,
      "step": 1050
    },
    {
      "epoch": 1.4866760168302946,
      "grad_norm": 2.496697425842285,
      "learning_rate": 1.0088826554464703e-05,
      "loss": 0.6649,
      "step": 1060
    },
    {
      "epoch": 1.5007012622720897,
      "grad_norm": 4.378758907318115,
      "learning_rate": 9.99532491818607e-06,
      "loss": 0.6614,
      "step": 1070
    },
    {
      "epoch": 1.514726507713885,
      "grad_norm": 0.9456027150154114,
      "learning_rate": 9.901823281907434e-06,
      "loss": 0.5802,
      "step": 1080
    },
    {
      "epoch": 1.5287517531556802,
      "grad_norm": 1.3726744651794434,
      "learning_rate": 9.808321645628799e-06,
      "loss": 0.5934,
      "step": 1090
    },
    {
      "epoch": 1.5427769985974753,
      "grad_norm": 1.352730631828308,
      "learning_rate": 9.714820009350165e-06,
      "loss": 0.5348,
      "step": 1100
    },
    {
      "epoch": 1.5568022440392708,
      "grad_norm": 1.5641446113586426,
      "learning_rate": 9.62131837307153e-06,
      "loss": 0.6208,
      "step": 1110
    },
    {
      "epoch": 1.5708274894810659,
      "grad_norm": 2.8659887313842773,
      "learning_rate": 9.527816736792894e-06,
      "loss": 0.6464,
      "step": 1120
    },
    {
      "epoch": 1.5848527349228612,
      "grad_norm": 2.025719404220581,
      "learning_rate": 9.434315100514259e-06,
      "loss": 0.6171,
      "step": 1130
    },
    {
      "epoch": 1.5988779803646564,
      "grad_norm": 2.403022527694702,
      "learning_rate": 9.340813464235625e-06,
      "loss": 0.5967,
      "step": 1140
    },
    {
      "epoch": 1.6129032258064515,
      "grad_norm": 1.6642073392868042,
      "learning_rate": 9.24731182795699e-06,
      "loss": 0.6299,
      "step": 1150
    },
    {
      "epoch": 1.6269284712482468,
      "grad_norm": 1.8569364547729492,
      "learning_rate": 9.153810191678355e-06,
      "loss": 0.5075,
      "step": 1160
    },
    {
      "epoch": 1.640953716690042,
      "grad_norm": 1.6565032005310059,
      "learning_rate": 9.060308555399721e-06,
      "loss": 0.6496,
      "step": 1170
    },
    {
      "epoch": 1.6549789621318372,
      "grad_norm": 1.9214714765548706,
      "learning_rate": 8.966806919121086e-06,
      "loss": 0.5989,
      "step": 1180
    },
    {
      "epoch": 1.6690042075736327,
      "grad_norm": 2.543281078338623,
      "learning_rate": 8.87330528284245e-06,
      "loss": 0.5836,
      "step": 1190
    },
    {
      "epoch": 1.6830294530154277,
      "grad_norm": 2.0761148929595947,
      "learning_rate": 8.779803646563817e-06,
      "loss": 0.6104,
      "step": 1200
    },
    {
      "epoch": 1.697054698457223,
      "grad_norm": 4.62771463394165,
      "learning_rate": 8.68630201028518e-06,
      "loss": 0.6927,
      "step": 1210
    },
    {
      "epoch": 1.7110799438990183,
      "grad_norm": 1.810335636138916,
      "learning_rate": 8.592800374006546e-06,
      "loss": 0.5861,
      "step": 1220
    },
    {
      "epoch": 1.7251051893408134,
      "grad_norm": 4.6862030029296875,
      "learning_rate": 8.49929873772791e-06,
      "loss": 0.7162,
      "step": 1230
    },
    {
      "epoch": 1.7391304347826086,
      "grad_norm": 2.0541458129882812,
      "learning_rate": 8.405797101449275e-06,
      "loss": 0.6332,
      "step": 1240
    },
    {
      "epoch": 1.753155680224404,
      "grad_norm": 3.4274206161499023,
      "learning_rate": 8.312295465170642e-06,
      "loss": 0.6248,
      "step": 1250
    },
    {
      "epoch": 1.767180925666199,
      "grad_norm": 4.098411560058594,
      "learning_rate": 8.218793828892006e-06,
      "loss": 0.5071,
      "step": 1260
    },
    {
      "epoch": 1.7812061711079945,
      "grad_norm": 1.9112437963485718,
      "learning_rate": 8.125292192613371e-06,
      "loss": 0.6055,
      "step": 1270
    },
    {
      "epoch": 1.7952314165497896,
      "grad_norm": 1.7826359272003174,
      "learning_rate": 8.031790556334737e-06,
      "loss": 0.6485,
      "step": 1280
    },
    {
      "epoch": 1.8092566619915849,
      "grad_norm": 1.7658352851867676,
      "learning_rate": 7.938288920056102e-06,
      "loss": 0.6823,
      "step": 1290
    },
    {
      "epoch": 1.8232819074333801,
      "grad_norm": 2.7262542247772217,
      "learning_rate": 7.844787283777467e-06,
      "loss": 0.5737,
      "step": 1300
    },
    {
      "epoch": 1.8373071528751752,
      "grad_norm": 1.817017674446106,
      "learning_rate": 7.751285647498831e-06,
      "loss": 0.6233,
      "step": 1310
    },
    {
      "epoch": 1.8513323983169705,
      "grad_norm": 2.0794713497161865,
      "learning_rate": 7.657784011220198e-06,
      "loss": 0.6131,
      "step": 1320
    },
    {
      "epoch": 1.8653576437587658,
      "grad_norm": 12.73442554473877,
      "learning_rate": 7.564282374941561e-06,
      "loss": 0.6117,
      "step": 1330
    },
    {
      "epoch": 1.8793828892005608,
      "grad_norm": 4.515372276306152,
      "learning_rate": 7.470780738662927e-06,
      "loss": 0.6121,
      "step": 1340
    },
    {
      "epoch": 1.8934081346423564,
      "grad_norm": 5.619593620300293,
      "learning_rate": 7.377279102384292e-06,
      "loss": 0.652,
      "step": 1350
    },
    {
      "epoch": 1.9074333800841514,
      "grad_norm": 1.645768404006958,
      "learning_rate": 7.283777466105657e-06,
      "loss": 0.6145,
      "step": 1360
    },
    {
      "epoch": 1.9214586255259467,
      "grad_norm": 3.058868169784546,
      "learning_rate": 7.1902758298270225e-06,
      "loss": 0.5594,
      "step": 1370
    },
    {
      "epoch": 1.935483870967742,
      "grad_norm": 1.8238847255706787,
      "learning_rate": 7.096774193548388e-06,
      "loss": 0.6179,
      "step": 1380
    },
    {
      "epoch": 1.949509116409537,
      "grad_norm": 1.8612594604492188,
      "learning_rate": 7.003272557269753e-06,
      "loss": 0.5943,
      "step": 1390
    },
    {
      "epoch": 1.9635343618513323,
      "grad_norm": 5.469630241394043,
      "learning_rate": 6.909770920991118e-06,
      "loss": 0.6528,
      "step": 1400
    },
    {
      "epoch": 1.9775596072931276,
      "grad_norm": 1.9232207536697388,
      "learning_rate": 6.816269284712484e-06,
      "loss": 0.545,
      "step": 1410
    },
    {
      "epoch": 1.9915848527349227,
      "grad_norm": 5.199339389801025,
      "learning_rate": 6.7227676484338475e-06,
      "loss": 0.7071,
      "step": 1420
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.6719242902208202,
      "eval_f1": 0.6280892650379813,
      "eval_loss": 0.628874659538269,
      "eval_precision": 0.6788527391906028,
      "eval_recall": 0.6719242902208202,
      "eval_runtime": 22.8133,
      "eval_samples_per_second": 27.791,
      "eval_steps_per_second": 3.507,
      "step": 1426
    },
    {
      "epoch": 2.005610098176718,
      "grad_norm": 2.3958261013031006,
      "learning_rate": 6.629266012155213e-06,
      "loss": 0.645,
      "step": 1430
    },
    {
      "epoch": 2.0196353436185133,
      "grad_norm": 2.5296549797058105,
      "learning_rate": 6.5357643758765785e-06,
      "loss": 0.624,
      "step": 1440
    },
    {
      "epoch": 2.0336605890603083,
      "grad_norm": 1.7562121152877808,
      "learning_rate": 6.442262739597943e-06,
      "loss": 0.593,
      "step": 1450
    },
    {
      "epoch": 2.047685834502104,
      "grad_norm": 1.5120015144348145,
      "learning_rate": 6.348761103319309e-06,
      "loss": 0.7043,
      "step": 1460
    },
    {
      "epoch": 2.061711079943899,
      "grad_norm": 3.399845838546753,
      "learning_rate": 6.255259467040674e-06,
      "loss": 0.5725,
      "step": 1470
    },
    {
      "epoch": 2.0757363253856944,
      "grad_norm": 1.333851933479309,
      "learning_rate": 6.161757830762039e-06,
      "loss": 0.6179,
      "step": 1480
    },
    {
      "epoch": 2.0897615708274895,
      "grad_norm": 6.548900127410889,
      "learning_rate": 6.068256194483404e-06,
      "loss": 0.6373,
      "step": 1490
    },
    {
      "epoch": 2.1037868162692845,
      "grad_norm": 1.415789008140564,
      "learning_rate": 5.97475455820477e-06,
      "loss": 0.6011,
      "step": 1500
    },
    {
      "epoch": 2.11781206171108,
      "grad_norm": 3.589219331741333,
      "learning_rate": 5.881252921926134e-06,
      "loss": 0.5983,
      "step": 1510
    },
    {
      "epoch": 2.131837307152875,
      "grad_norm": 2.495122194290161,
      "learning_rate": 5.787751285647499e-06,
      "loss": 0.6909,
      "step": 1520
    },
    {
      "epoch": 2.1458625525946706,
      "grad_norm": 2.209118127822876,
      "learning_rate": 5.694249649368865e-06,
      "loss": 0.6925,
      "step": 1530
    },
    {
      "epoch": 2.1598877980364657,
      "grad_norm": 1.1108182668685913,
      "learning_rate": 5.600748013090229e-06,
      "loss": 0.6666,
      "step": 1540
    },
    {
      "epoch": 2.1739130434782608,
      "grad_norm": 2.0759012699127197,
      "learning_rate": 5.507246376811595e-06,
      "loss": 0.6255,
      "step": 1550
    },
    {
      "epoch": 2.1879382889200563,
      "grad_norm": 1.3523576259613037,
      "learning_rate": 5.41374474053296e-06,
      "loss": 0.6818,
      "step": 1560
    },
    {
      "epoch": 2.2019635343618513,
      "grad_norm": 93.50439453125,
      "learning_rate": 5.320243104254324e-06,
      "loss": 0.6811,
      "step": 1570
    },
    {
      "epoch": 2.2159887798036464,
      "grad_norm": 1.6024489402770996,
      "learning_rate": 5.22674146797569e-06,
      "loss": 0.6357,
      "step": 1580
    },
    {
      "epoch": 2.230014025245442,
      "grad_norm": 1.4730666875839233,
      "learning_rate": 5.133239831697055e-06,
      "loss": 0.6033,
      "step": 1590
    },
    {
      "epoch": 2.244039270687237,
      "grad_norm": 1.618535041809082,
      "learning_rate": 5.03973819541842e-06,
      "loss": 0.6385,
      "step": 1600
    },
    {
      "epoch": 2.258064516129032,
      "grad_norm": 3.20149564743042,
      "learning_rate": 4.946236559139785e-06,
      "loss": 0.6185,
      "step": 1610
    },
    {
      "epoch": 2.2720897615708275,
      "grad_norm": 1.5379351377487183,
      "learning_rate": 4.85273492286115e-06,
      "loss": 0.5394,
      "step": 1620
    },
    {
      "epoch": 2.2861150070126226,
      "grad_norm": 8.711630821228027,
      "learning_rate": 4.7592332865825155e-06,
      "loss": 0.5705,
      "step": 1630
    },
    {
      "epoch": 2.300140252454418,
      "grad_norm": 1.4725191593170166,
      "learning_rate": 4.665731650303881e-06,
      "loss": 0.6725,
      "step": 1640
    },
    {
      "epoch": 2.314165497896213,
      "grad_norm": 1.7263610363006592,
      "learning_rate": 4.572230014025246e-06,
      "loss": 0.6521,
      "step": 1650
    },
    {
      "epoch": 2.3281907433380082,
      "grad_norm": 3.538235902786255,
      "learning_rate": 4.478728377746611e-06,
      "loss": 0.6116,
      "step": 1660
    },
    {
      "epoch": 2.3422159887798037,
      "grad_norm": 3.5811495780944824,
      "learning_rate": 4.385226741467976e-06,
      "loss": 0.5776,
      "step": 1670
    },
    {
      "epoch": 2.356241234221599,
      "grad_norm": 4.654789447784424,
      "learning_rate": 4.291725105189341e-06,
      "loss": 0.6173,
      "step": 1680
    },
    {
      "epoch": 2.3702664796633943,
      "grad_norm": 4.460419654846191,
      "learning_rate": 4.198223468910707e-06,
      "loss": 0.7286,
      "step": 1690
    },
    {
      "epoch": 2.3842917251051894,
      "grad_norm": 2.130150556564331,
      "learning_rate": 4.1047218326320715e-06,
      "loss": 0.638,
      "step": 1700
    },
    {
      "epoch": 2.3983169705469845,
      "grad_norm": 1.312013030052185,
      "learning_rate": 4.011220196353436e-06,
      "loss": 0.6493,
      "step": 1710
    },
    {
      "epoch": 2.41234221598878,
      "grad_norm": 2.6195452213287354,
      "learning_rate": 3.917718560074802e-06,
      "loss": 0.6488,
      "step": 1720
    },
    {
      "epoch": 2.426367461430575,
      "grad_norm": 2.6287496089935303,
      "learning_rate": 3.824216923796167e-06,
      "loss": 0.665,
      "step": 1730
    },
    {
      "epoch": 2.44039270687237,
      "grad_norm": 1.6869184970855713,
      "learning_rate": 3.730715287517532e-06,
      "loss": 0.5519,
      "step": 1740
    },
    {
      "epoch": 2.4544179523141656,
      "grad_norm": 3.2667226791381836,
      "learning_rate": 3.6372136512388973e-06,
      "loss": 0.5385,
      "step": 1750
    },
    {
      "epoch": 2.4684431977559607,
      "grad_norm": 1.8451693058013916,
      "learning_rate": 3.543712014960262e-06,
      "loss": 0.6043,
      "step": 1760
    },
    {
      "epoch": 2.4824684431977557,
      "grad_norm": 2.265134334564209,
      "learning_rate": 3.450210378681627e-06,
      "loss": 0.6408,
      "step": 1770
    },
    {
      "epoch": 2.4964936886395512,
      "grad_norm": 2.8493213653564453,
      "learning_rate": 3.3567087424029926e-06,
      "loss": 0.6559,
      "step": 1780
    },
    {
      "epoch": 2.5105189340813463,
      "grad_norm": 3.460724115371704,
      "learning_rate": 3.2632071061243577e-06,
      "loss": 0.6332,
      "step": 1790
    },
    {
      "epoch": 2.524544179523142,
      "grad_norm": 2.583472490310669,
      "learning_rate": 3.1697054698457223e-06,
      "loss": 0.6658,
      "step": 1800
    },
    {
      "epoch": 2.538569424964937,
      "grad_norm": 2.256755828857422,
      "learning_rate": 3.0762038335670874e-06,
      "loss": 0.6081,
      "step": 1810
    },
    {
      "epoch": 2.552594670406732,
      "grad_norm": 1.5637458562850952,
      "learning_rate": 2.982702197288453e-06,
      "loss": 0.6722,
      "step": 1820
    },
    {
      "epoch": 2.5666199158485274,
      "grad_norm": 2.259047746658325,
      "learning_rate": 2.889200561009818e-06,
      "loss": 0.6407,
      "step": 1830
    },
    {
      "epoch": 2.5806451612903225,
      "grad_norm": 3.4032070636749268,
      "learning_rate": 2.7956989247311827e-06,
      "loss": 0.6402,
      "step": 1840
    },
    {
      "epoch": 2.594670406732118,
      "grad_norm": 2.416168689727783,
      "learning_rate": 2.702197288452548e-06,
      "loss": 0.6202,
      "step": 1850
    },
    {
      "epoch": 2.608695652173913,
      "grad_norm": 1.9406838417053223,
      "learning_rate": 2.6086956521739132e-06,
      "loss": 0.6743,
      "step": 1860
    },
    {
      "epoch": 2.622720897615708,
      "grad_norm": 1.679682731628418,
      "learning_rate": 2.5151940158952783e-06,
      "loss": 0.5763,
      "step": 1870
    },
    {
      "epoch": 2.6367461430575037,
      "grad_norm": 1.8721973896026611,
      "learning_rate": 2.4216923796166434e-06,
      "loss": 0.6342,
      "step": 1880
    },
    {
      "epoch": 2.6507713884992987,
      "grad_norm": 3.9420793056488037,
      "learning_rate": 2.3281907433380085e-06,
      "loss": 0.6058,
      "step": 1890
    },
    {
      "epoch": 2.6647966339410942,
      "grad_norm": 6.487459182739258,
      "learning_rate": 2.2346891070593736e-06,
      "loss": 0.6021,
      "step": 1900
    },
    {
      "epoch": 2.6788218793828893,
      "grad_norm": 1.5586847066879272,
      "learning_rate": 2.141187470780739e-06,
      "loss": 0.625,
      "step": 1910
    },
    {
      "epoch": 2.6928471248246844,
      "grad_norm": 1.7292633056640625,
      "learning_rate": 2.0476858345021037e-06,
      "loss": 0.5582,
      "step": 1920
    },
    {
      "epoch": 2.7068723702664794,
      "grad_norm": 1.1768126487731934,
      "learning_rate": 1.9541841982234692e-06,
      "loss": 0.6239,
      "step": 1930
    },
    {
      "epoch": 2.720897615708275,
      "grad_norm": 2.9500701427459717,
      "learning_rate": 1.8606825619448343e-06,
      "loss": 0.663,
      "step": 1940
    },
    {
      "epoch": 2.73492286115007,
      "grad_norm": 5.452463626861572,
      "learning_rate": 1.7671809256661992e-06,
      "loss": 0.7494,
      "step": 1950
    },
    {
      "epoch": 2.7489481065918655,
      "grad_norm": 3.2315194606781006,
      "learning_rate": 1.6736792893875645e-06,
      "loss": 0.5578,
      "step": 1960
    },
    {
      "epoch": 2.7629733520336606,
      "grad_norm": 2.7353618144989014,
      "learning_rate": 1.5801776531089294e-06,
      "loss": 0.6591,
      "step": 1970
    },
    {
      "epoch": 2.7769985974754556,
      "grad_norm": 5.821049690246582,
      "learning_rate": 1.4866760168302946e-06,
      "loss": 0.6131,
      "step": 1980
    },
    {
      "epoch": 2.791023842917251,
      "grad_norm": 2.649395227432251,
      "learning_rate": 1.3931743805516597e-06,
      "loss": 0.6538,
      "step": 1990
    },
    {
      "epoch": 2.805049088359046,
      "grad_norm": 2.995612144470215,
      "learning_rate": 1.2996727442730248e-06,
      "loss": 0.6074,
      "step": 2000
    },
    {
      "epoch": 2.8190743338008417,
      "grad_norm": 5.988626003265381,
      "learning_rate": 1.2061711079943899e-06,
      "loss": 0.59,
      "step": 2010
    },
    {
      "epoch": 2.833099579242637,
      "grad_norm": 3.201739549636841,
      "learning_rate": 1.1126694717157552e-06,
      "loss": 0.5955,
      "step": 2020
    },
    {
      "epoch": 2.847124824684432,
      "grad_norm": 2.220841646194458,
      "learning_rate": 1.0191678354371203e-06,
      "loss": 0.5688,
      "step": 2030
    },
    {
      "epoch": 2.8611500701262274,
      "grad_norm": 6.18785285949707,
      "learning_rate": 9.256661991584853e-07,
      "loss": 0.5901,
      "step": 2040
    },
    {
      "epoch": 2.8751753155680224,
      "grad_norm": 1.5058602094650269,
      "learning_rate": 8.321645628798504e-07,
      "loss": 0.5891,
      "step": 2050
    },
    {
      "epoch": 2.889200561009818,
      "grad_norm": 2.963960647583008,
      "learning_rate": 7.386629266012156e-07,
      "loss": 0.5464,
      "step": 2060
    },
    {
      "epoch": 2.903225806451613,
      "grad_norm": 3.6054282188415527,
      "learning_rate": 6.451612903225807e-07,
      "loss": 0.6938,
      "step": 2070
    },
    {
      "epoch": 2.917251051893408,
      "grad_norm": 1.3518725633621216,
      "learning_rate": 5.516596540439458e-07,
      "loss": 0.5411,
      "step": 2080
    },
    {
      "epoch": 2.931276297335203,
      "grad_norm": 2.6974754333496094,
      "learning_rate": 4.5815801776531097e-07,
      "loss": 0.616,
      "step": 2090
    },
    {
      "epoch": 2.9453015427769986,
      "grad_norm": 9.552983283996582,
      "learning_rate": 3.6465638148667605e-07,
      "loss": 0.6325,
      "step": 2100
    },
    {
      "epoch": 2.9593267882187937,
      "grad_norm": 2.549330949783325,
      "learning_rate": 2.711547452080412e-07,
      "loss": 0.6205,
      "step": 2110
    },
    {
      "epoch": 2.973352033660589,
      "grad_norm": 1.4433825016021729,
      "learning_rate": 1.776531089294063e-07,
      "loss": 0.5882,
      "step": 2120
    },
    {
      "epoch": 2.9873772791023843,
      "grad_norm": 2.5889523029327393,
      "learning_rate": 8.415147265077139e-08,
      "loss": 0.6539,
      "step": 2130
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.667192429022082,
      "eval_f1": 0.6263677451424056,
      "eval_loss": 0.6233745217323303,
      "eval_precision": 0.6678917921552289,
      "eval_recall": 0.667192429022082,
      "eval_runtime": 22.8266,
      "eval_samples_per_second": 27.775,
      "eval_steps_per_second": 3.505,
      "step": 2139
    }
  ],
  "logging_steps": 10,
  "max_steps": 2139,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4561288508620800.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
