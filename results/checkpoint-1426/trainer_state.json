{
  "best_metric": 0.6719242902208202,
  "best_model_checkpoint": "./results/checkpoint-1426",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 1426,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.014025245441795231,
      "grad_norm": 1.8241004943847656,
      "learning_rate": 1.9906498363721366e-05,
      "loss": 0.7143,
      "step": 10
    },
    {
      "epoch": 0.028050490883590462,
      "grad_norm": 1.8873220682144165,
      "learning_rate": 1.981299672744273e-05,
      "loss": 0.7019,
      "step": 20
    },
    {
      "epoch": 0.04207573632538569,
      "grad_norm": 3.033406972885132,
      "learning_rate": 1.9719495091164096e-05,
      "loss": 0.6907,
      "step": 30
    },
    {
      "epoch": 0.056100981767180924,
      "grad_norm": 2.5807647705078125,
      "learning_rate": 1.962599345488546e-05,
      "loss": 0.6914,
      "step": 40
    },
    {
      "epoch": 0.07012622720897616,
      "grad_norm": 1.642714023590088,
      "learning_rate": 1.953249181860683e-05,
      "loss": 0.6849,
      "step": 50
    },
    {
      "epoch": 0.08415147265077139,
      "grad_norm": 1.501880168914795,
      "learning_rate": 1.9438990182328193e-05,
      "loss": 0.6948,
      "step": 60
    },
    {
      "epoch": 0.09817671809256662,
      "grad_norm": 5.693972587585449,
      "learning_rate": 1.9345488546049558e-05,
      "loss": 0.6657,
      "step": 70
    },
    {
      "epoch": 0.11220196353436185,
      "grad_norm": 1.1748579740524292,
      "learning_rate": 1.9251986909770922e-05,
      "loss": 0.6804,
      "step": 80
    },
    {
      "epoch": 0.12622720897615708,
      "grad_norm": 1.280032753944397,
      "learning_rate": 1.9158485273492287e-05,
      "loss": 0.6723,
      "step": 90
    },
    {
      "epoch": 0.1402524544179523,
      "grad_norm": 2.26155948638916,
      "learning_rate": 1.906498363721365e-05,
      "loss": 0.6538,
      "step": 100
    },
    {
      "epoch": 0.15427769985974754,
      "grad_norm": 1.3195356130599976,
      "learning_rate": 1.8971482000935016e-05,
      "loss": 0.7061,
      "step": 110
    },
    {
      "epoch": 0.16830294530154277,
      "grad_norm": 1.364357590675354,
      "learning_rate": 1.8877980364656384e-05,
      "loss": 0.6547,
      "step": 120
    },
    {
      "epoch": 0.182328190743338,
      "grad_norm": 1.9039897918701172,
      "learning_rate": 1.878447872837775e-05,
      "loss": 0.6562,
      "step": 130
    },
    {
      "epoch": 0.19635343618513323,
      "grad_norm": 1.217930793762207,
      "learning_rate": 1.8690977092099114e-05,
      "loss": 0.7041,
      "step": 140
    },
    {
      "epoch": 0.21037868162692847,
      "grad_norm": 1.0880048274993896,
      "learning_rate": 1.8597475455820478e-05,
      "loss": 0.661,
      "step": 150
    },
    {
      "epoch": 0.2244039270687237,
      "grad_norm": 3.6458892822265625,
      "learning_rate": 1.8503973819541843e-05,
      "loss": 0.6685,
      "step": 160
    },
    {
      "epoch": 0.23842917251051893,
      "grad_norm": 1.855943202972412,
      "learning_rate": 1.8410472183263208e-05,
      "loss": 0.6653,
      "step": 170
    },
    {
      "epoch": 0.25245441795231416,
      "grad_norm": 1.1566880941390991,
      "learning_rate": 1.8316970546984572e-05,
      "loss": 0.6559,
      "step": 180
    },
    {
      "epoch": 0.2664796633941094,
      "grad_norm": 1.606073021888733,
      "learning_rate": 1.822346891070594e-05,
      "loss": 0.6484,
      "step": 190
    },
    {
      "epoch": 0.2805049088359046,
      "grad_norm": 0.7722606658935547,
      "learning_rate": 1.8129967274427305e-05,
      "loss": 0.6548,
      "step": 200
    },
    {
      "epoch": 0.29453015427769985,
      "grad_norm": 2.0082902908325195,
      "learning_rate": 1.803646563814867e-05,
      "loss": 0.6143,
      "step": 210
    },
    {
      "epoch": 0.3085553997194951,
      "grad_norm": 1.231088638305664,
      "learning_rate": 1.7942964001870034e-05,
      "loss": 0.6844,
      "step": 220
    },
    {
      "epoch": 0.3225806451612903,
      "grad_norm": 2.558121919631958,
      "learning_rate": 1.78494623655914e-05,
      "loss": 0.6452,
      "step": 230
    },
    {
      "epoch": 0.33660589060308554,
      "grad_norm": 1.3861174583435059,
      "learning_rate": 1.7755960729312764e-05,
      "loss": 0.6319,
      "step": 240
    },
    {
      "epoch": 0.3506311360448808,
      "grad_norm": 1.1863765716552734,
      "learning_rate": 1.766245909303413e-05,
      "loss": 0.6894,
      "step": 250
    },
    {
      "epoch": 0.364656381486676,
      "grad_norm": 0.7706034183502197,
      "learning_rate": 1.7568957456755496e-05,
      "loss": 0.6843,
      "step": 260
    },
    {
      "epoch": 0.37868162692847124,
      "grad_norm": 4.820980548858643,
      "learning_rate": 1.7475455820476858e-05,
      "loss": 0.6859,
      "step": 270
    },
    {
      "epoch": 0.39270687237026647,
      "grad_norm": 4.558115482330322,
      "learning_rate": 1.7381954184198226e-05,
      "loss": 0.6081,
      "step": 280
    },
    {
      "epoch": 0.4067321178120617,
      "grad_norm": 1.2031550407409668,
      "learning_rate": 1.728845254791959e-05,
      "loss": 0.6268,
      "step": 290
    },
    {
      "epoch": 0.42075736325385693,
      "grad_norm": 1.2579007148742676,
      "learning_rate": 1.7194950911640955e-05,
      "loss": 0.6954,
      "step": 300
    },
    {
      "epoch": 0.43478260869565216,
      "grad_norm": 0.9554207921028137,
      "learning_rate": 1.710144927536232e-05,
      "loss": 0.6831,
      "step": 310
    },
    {
      "epoch": 0.4488078541374474,
      "grad_norm": 1.7102313041687012,
      "learning_rate": 1.7007947639083688e-05,
      "loss": 0.6558,
      "step": 320
    },
    {
      "epoch": 0.4628330995792426,
      "grad_norm": 1.115724802017212,
      "learning_rate": 1.691444600280505e-05,
      "loss": 0.6881,
      "step": 330
    },
    {
      "epoch": 0.47685834502103785,
      "grad_norm": 2.0797879695892334,
      "learning_rate": 1.6820944366526414e-05,
      "loss": 0.6847,
      "step": 340
    },
    {
      "epoch": 0.4908835904628331,
      "grad_norm": 1.713870644569397,
      "learning_rate": 1.672744273024778e-05,
      "loss": 0.6637,
      "step": 350
    },
    {
      "epoch": 0.5049088359046283,
      "grad_norm": 1.2933088541030884,
      "learning_rate": 1.6633941093969146e-05,
      "loss": 0.6503,
      "step": 360
    },
    {
      "epoch": 0.5189340813464236,
      "grad_norm": 1.1933996677398682,
      "learning_rate": 1.654043945769051e-05,
      "loss": 0.6258,
      "step": 370
    },
    {
      "epoch": 0.5329593267882188,
      "grad_norm": 1.4887624979019165,
      "learning_rate": 1.6446937821411876e-05,
      "loss": 0.6484,
      "step": 380
    },
    {
      "epoch": 0.5469845722300141,
      "grad_norm": 1.7436931133270264,
      "learning_rate": 1.635343618513324e-05,
      "loss": 0.7026,
      "step": 390
    },
    {
      "epoch": 0.5610098176718092,
      "grad_norm": 1.5043095350265503,
      "learning_rate": 1.6259934548854605e-05,
      "loss": 0.6496,
      "step": 400
    },
    {
      "epoch": 0.5750350631136045,
      "grad_norm": 0.9189023971557617,
      "learning_rate": 1.616643291257597e-05,
      "loss": 0.6362,
      "step": 410
    },
    {
      "epoch": 0.5890603085553997,
      "grad_norm": 1.821621298789978,
      "learning_rate": 1.6072931276297338e-05,
      "loss": 0.6074,
      "step": 420
    },
    {
      "epoch": 0.603085553997195,
      "grad_norm": 4.068702697753906,
      "learning_rate": 1.5979429640018702e-05,
      "loss": 0.6803,
      "step": 430
    },
    {
      "epoch": 0.6171107994389902,
      "grad_norm": 1.8756545782089233,
      "learning_rate": 1.5885928003740067e-05,
      "loss": 0.6737,
      "step": 440
    },
    {
      "epoch": 0.6311360448807855,
      "grad_norm": 1.2406938076019287,
      "learning_rate": 1.579242636746143e-05,
      "loss": 0.6328,
      "step": 450
    },
    {
      "epoch": 0.6451612903225806,
      "grad_norm": 3.460902214050293,
      "learning_rate": 1.5698924731182796e-05,
      "loss": 0.6345,
      "step": 460
    },
    {
      "epoch": 0.6591865357643759,
      "grad_norm": 1.8463013172149658,
      "learning_rate": 1.560542309490416e-05,
      "loss": 0.6406,
      "step": 470
    },
    {
      "epoch": 0.6732117812061711,
      "grad_norm": 1.627772331237793,
      "learning_rate": 1.551192145862553e-05,
      "loss": 0.5544,
      "step": 480
    },
    {
      "epoch": 0.6872370266479664,
      "grad_norm": 1.3952943086624146,
      "learning_rate": 1.5418419822346894e-05,
      "loss": 0.6567,
      "step": 490
    },
    {
      "epoch": 0.7012622720897616,
      "grad_norm": 1.5266954898834229,
      "learning_rate": 1.5324918186068258e-05,
      "loss": 0.6566,
      "step": 500
    },
    {
      "epoch": 0.7152875175315568,
      "grad_norm": 1.5833094120025635,
      "learning_rate": 1.5231416549789621e-05,
      "loss": 0.7092,
      "step": 510
    },
    {
      "epoch": 0.729312762973352,
      "grad_norm": 3.505596876144409,
      "learning_rate": 1.5137914913510988e-05,
      "loss": 0.6731,
      "step": 520
    },
    {
      "epoch": 0.7433380084151473,
      "grad_norm": 3.6939525604248047,
      "learning_rate": 1.5044413277232352e-05,
      "loss": 0.5616,
      "step": 530
    },
    {
      "epoch": 0.7573632538569425,
      "grad_norm": 16.431257247924805,
      "learning_rate": 1.4950911640953719e-05,
      "loss": 0.5385,
      "step": 540
    },
    {
      "epoch": 0.7713884992987378,
      "grad_norm": 1.773467779159546,
      "learning_rate": 1.4857410004675083e-05,
      "loss": 0.6231,
      "step": 550
    },
    {
      "epoch": 0.7854137447405329,
      "grad_norm": 5.8781657218933105,
      "learning_rate": 1.476390836839645e-05,
      "loss": 0.5992,
      "step": 560
    },
    {
      "epoch": 0.7994389901823282,
      "grad_norm": 3.7641568183898926,
      "learning_rate": 1.4670406732117813e-05,
      "loss": 0.5323,
      "step": 570
    },
    {
      "epoch": 0.8134642356241234,
      "grad_norm": 6.7463908195495605,
      "learning_rate": 1.4576905095839177e-05,
      "loss": 0.5585,
      "step": 580
    },
    {
      "epoch": 0.8274894810659187,
      "grad_norm": 1.9736450910568237,
      "learning_rate": 1.4483403459560544e-05,
      "loss": 0.6094,
      "step": 590
    },
    {
      "epoch": 0.8415147265077139,
      "grad_norm": 4.501043796539307,
      "learning_rate": 1.4389901823281908e-05,
      "loss": 0.6651,
      "step": 600
    },
    {
      "epoch": 0.8555399719495091,
      "grad_norm": 1.606547236442566,
      "learning_rate": 1.4296400187003275e-05,
      "loss": 0.5234,
      "step": 610
    },
    {
      "epoch": 0.8695652173913043,
      "grad_norm": 4.599311828613281,
      "learning_rate": 1.420289855072464e-05,
      "loss": 0.5859,
      "step": 620
    },
    {
      "epoch": 0.8835904628330996,
      "grad_norm": 1.7662242650985718,
      "learning_rate": 1.4109396914446002e-05,
      "loss": 0.5484,
      "step": 630
    },
    {
      "epoch": 0.8976157082748948,
      "grad_norm": 1.4752923250198364,
      "learning_rate": 1.4015895278167369e-05,
      "loss": 0.6384,
      "step": 640
    },
    {
      "epoch": 0.9116409537166901,
      "grad_norm": 3.507154941558838,
      "learning_rate": 1.3922393641888733e-05,
      "loss": 0.5926,
      "step": 650
    },
    {
      "epoch": 0.9256661991584852,
      "grad_norm": 4.35238790512085,
      "learning_rate": 1.38288920056101e-05,
      "loss": 0.6767,
      "step": 660
    },
    {
      "epoch": 0.9396914446002805,
      "grad_norm": 7.9265313148498535,
      "learning_rate": 1.3735390369331464e-05,
      "loss": 0.6994,
      "step": 670
    },
    {
      "epoch": 0.9537166900420757,
      "grad_norm": 4.993457317352295,
      "learning_rate": 1.364188873305283e-05,
      "loss": 0.6325,
      "step": 680
    },
    {
      "epoch": 0.967741935483871,
      "grad_norm": 2.273259162902832,
      "learning_rate": 1.3548387096774194e-05,
      "loss": 0.6709,
      "step": 690
    },
    {
      "epoch": 0.9817671809256662,
      "grad_norm": 2.7344272136688232,
      "learning_rate": 1.345488546049556e-05,
      "loss": 0.6485,
      "step": 700
    },
    {
      "epoch": 0.9957924263674615,
      "grad_norm": 3.6143453121185303,
      "learning_rate": 1.3361383824216925e-05,
      "loss": 0.6502,
      "step": 710
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.6656151419558359,
      "eval_f1": 0.6261034480185506,
      "eval_loss": 0.6324409246444702,
      "eval_precision": 0.6642838537898301,
      "eval_recall": 0.6656151419558359,
      "eval_runtime": 23.9649,
      "eval_samples_per_second": 26.455,
      "eval_steps_per_second": 3.338,
      "step": 713
    },
    {
      "epoch": 1.0098176718092566,
      "grad_norm": 1.3729970455169678,
      "learning_rate": 1.3267882187938291e-05,
      "loss": 0.7298,
      "step": 720
    },
    {
      "epoch": 1.023842917251052,
      "grad_norm": 5.296571254730225,
      "learning_rate": 1.3174380551659656e-05,
      "loss": 0.7004,
      "step": 730
    },
    {
      "epoch": 1.0378681626928472,
      "grad_norm": 2.650068759918213,
      "learning_rate": 1.3080878915381022e-05,
      "loss": 0.6042,
      "step": 740
    },
    {
      "epoch": 1.0518934081346423,
      "grad_norm": 4.230722427368164,
      "learning_rate": 1.2987377279102385e-05,
      "loss": 0.6418,
      "step": 750
    },
    {
      "epoch": 1.0659186535764376,
      "grad_norm": 4.734264850616455,
      "learning_rate": 1.289387564282375e-05,
      "loss": 0.6955,
      "step": 760
    },
    {
      "epoch": 1.0799438990182328,
      "grad_norm": 3.459261417388916,
      "learning_rate": 1.2800374006545116e-05,
      "loss": 0.5817,
      "step": 770
    },
    {
      "epoch": 1.0939691444600281,
      "grad_norm": 7.667307376861572,
      "learning_rate": 1.270687237026648e-05,
      "loss": 0.6905,
      "step": 780
    },
    {
      "epoch": 1.1079943899018232,
      "grad_norm": 1.966726541519165,
      "learning_rate": 1.2613370733987847e-05,
      "loss": 0.6055,
      "step": 790
    },
    {
      "epoch": 1.1220196353436185,
      "grad_norm": 3.073134422302246,
      "learning_rate": 1.2519869097709212e-05,
      "loss": 0.5784,
      "step": 800
    },
    {
      "epoch": 1.1360448807854138,
      "grad_norm": 2.5168581008911133,
      "learning_rate": 1.2426367461430575e-05,
      "loss": 0.6505,
      "step": 810
    },
    {
      "epoch": 1.150070126227209,
      "grad_norm": 4.882817268371582,
      "learning_rate": 1.2332865825151941e-05,
      "loss": 0.6365,
      "step": 820
    },
    {
      "epoch": 1.1640953716690041,
      "grad_norm": 2.9293594360351562,
      "learning_rate": 1.2239364188873306e-05,
      "loss": 0.6832,
      "step": 830
    },
    {
      "epoch": 1.1781206171107994,
      "grad_norm": 8.113359451293945,
      "learning_rate": 1.2145862552594672e-05,
      "loss": 0.6567,
      "step": 840
    },
    {
      "epoch": 1.1921458625525947,
      "grad_norm": 6.457350730895996,
      "learning_rate": 1.2052360916316037e-05,
      "loss": 0.5976,
      "step": 850
    },
    {
      "epoch": 1.20617110799439,
      "grad_norm": 2.3118393421173096,
      "learning_rate": 1.1958859280037403e-05,
      "loss": 0.69,
      "step": 860
    },
    {
      "epoch": 1.220196353436185,
      "grad_norm": 2.3140652179718018,
      "learning_rate": 1.1865357643758766e-05,
      "loss": 0.6297,
      "step": 870
    },
    {
      "epoch": 1.2342215988779803,
      "grad_norm": 1.8230854272842407,
      "learning_rate": 1.177185600748013e-05,
      "loss": 0.6959,
      "step": 880
    },
    {
      "epoch": 1.2482468443197756,
      "grad_norm": 2.8831725120544434,
      "learning_rate": 1.1678354371201497e-05,
      "loss": 0.7169,
      "step": 890
    },
    {
      "epoch": 1.262272089761571,
      "grad_norm": 1.2585653066635132,
      "learning_rate": 1.1584852734922862e-05,
      "loss": 0.5977,
      "step": 900
    },
    {
      "epoch": 1.276297335203366,
      "grad_norm": 2.536534309387207,
      "learning_rate": 1.1491351098644228e-05,
      "loss": 0.712,
      "step": 910
    },
    {
      "epoch": 1.2903225806451613,
      "grad_norm": 5.589643955230713,
      "learning_rate": 1.1397849462365593e-05,
      "loss": 0.601,
      "step": 920
    },
    {
      "epoch": 1.3043478260869565,
      "grad_norm": 1.7109798192977905,
      "learning_rate": 1.1304347826086957e-05,
      "loss": 0.5934,
      "step": 930
    },
    {
      "epoch": 1.3183730715287518,
      "grad_norm": 2.0838029384613037,
      "learning_rate": 1.1210846189808322e-05,
      "loss": 0.6015,
      "step": 940
    },
    {
      "epoch": 1.3323983169705471,
      "grad_norm": 2.4799795150756836,
      "learning_rate": 1.1117344553529688e-05,
      "loss": 0.6542,
      "step": 950
    },
    {
      "epoch": 1.3464235624123422,
      "grad_norm": 3.2704148292541504,
      "learning_rate": 1.1023842917251053e-05,
      "loss": 0.6366,
      "step": 960
    },
    {
      "epoch": 1.3604488078541375,
      "grad_norm": 5.000730037689209,
      "learning_rate": 1.093034128097242e-05,
      "loss": 0.6125,
      "step": 970
    },
    {
      "epoch": 1.3744740532959328,
      "grad_norm": 5.653116226196289,
      "learning_rate": 1.0836839644693784e-05,
      "loss": 0.5959,
      "step": 980
    },
    {
      "epoch": 1.3884992987377278,
      "grad_norm": 1.7434884309768677,
      "learning_rate": 1.0743338008415147e-05,
      "loss": 0.6712,
      "step": 990
    },
    {
      "epoch": 1.402524544179523,
      "grad_norm": 4.892682075500488,
      "learning_rate": 1.0649836372136513e-05,
      "loss": 0.6165,
      "step": 1000
    },
    {
      "epoch": 1.4165497896213184,
      "grad_norm": 1.523084282875061,
      "learning_rate": 1.0556334735857878e-05,
      "loss": 0.6107,
      "step": 1010
    },
    {
      "epoch": 1.4305750350631137,
      "grad_norm": 1.915809988975525,
      "learning_rate": 1.0462833099579244e-05,
      "loss": 0.5505,
      "step": 1020
    },
    {
      "epoch": 1.444600280504909,
      "grad_norm": 3.8985660076141357,
      "learning_rate": 1.0369331463300609e-05,
      "loss": 0.5913,
      "step": 1030
    },
    {
      "epoch": 1.458625525946704,
      "grad_norm": 2.087376356124878,
      "learning_rate": 1.0275829827021975e-05,
      "loss": 0.6582,
      "step": 1040
    },
    {
      "epoch": 1.4726507713884993,
      "grad_norm": 6.830135822296143,
      "learning_rate": 1.0182328190743338e-05,
      "loss": 0.6473,
      "step": 1050
    },
    {
      "epoch": 1.4866760168302946,
      "grad_norm": 2.6016411781311035,
      "learning_rate": 1.0088826554464703e-05,
      "loss": 0.6454,
      "step": 1060
    },
    {
      "epoch": 1.5007012622720897,
      "grad_norm": 4.557864189147949,
      "learning_rate": 9.99532491818607e-06,
      "loss": 0.6711,
      "step": 1070
    },
    {
      "epoch": 1.514726507713885,
      "grad_norm": 1.185027003288269,
      "learning_rate": 9.901823281907434e-06,
      "loss": 0.5809,
      "step": 1080
    },
    {
      "epoch": 1.5287517531556802,
      "grad_norm": 1.5268239974975586,
      "learning_rate": 9.808321645628799e-06,
      "loss": 0.5939,
      "step": 1090
    },
    {
      "epoch": 1.5427769985974753,
      "grad_norm": 1.3234050273895264,
      "learning_rate": 9.714820009350165e-06,
      "loss": 0.5416,
      "step": 1100
    },
    {
      "epoch": 1.5568022440392708,
      "grad_norm": 2.0370078086853027,
      "learning_rate": 9.62131837307153e-06,
      "loss": 0.5984,
      "step": 1110
    },
    {
      "epoch": 1.5708274894810659,
      "grad_norm": 2.901954412460327,
      "learning_rate": 9.527816736792894e-06,
      "loss": 0.6498,
      "step": 1120
    },
    {
      "epoch": 1.5848527349228612,
      "grad_norm": 5.099990367889404,
      "learning_rate": 9.434315100514259e-06,
      "loss": 0.6282,
      "step": 1130
    },
    {
      "epoch": 1.5988779803646564,
      "grad_norm": 2.5534842014312744,
      "learning_rate": 9.340813464235625e-06,
      "loss": 0.5989,
      "step": 1140
    },
    {
      "epoch": 1.6129032258064515,
      "grad_norm": 2.9885289669036865,
      "learning_rate": 9.24731182795699e-06,
      "loss": 0.6438,
      "step": 1150
    },
    {
      "epoch": 1.6269284712482468,
      "grad_norm": 2.1179935932159424,
      "learning_rate": 9.153810191678355e-06,
      "loss": 0.5192,
      "step": 1160
    },
    {
      "epoch": 1.640953716690042,
      "grad_norm": 1.4638921022415161,
      "learning_rate": 9.060308555399721e-06,
      "loss": 0.6543,
      "step": 1170
    },
    {
      "epoch": 1.6549789621318372,
      "grad_norm": 1.7697768211364746,
      "learning_rate": 8.966806919121086e-06,
      "loss": 0.605,
      "step": 1180
    },
    {
      "epoch": 1.6690042075736327,
      "grad_norm": 2.9291493892669678,
      "learning_rate": 8.87330528284245e-06,
      "loss": 0.5766,
      "step": 1190
    },
    {
      "epoch": 1.6830294530154277,
      "grad_norm": 2.3432347774505615,
      "learning_rate": 8.779803646563817e-06,
      "loss": 0.6144,
      "step": 1200
    },
    {
      "epoch": 1.697054698457223,
      "grad_norm": 4.357419490814209,
      "learning_rate": 8.68630201028518e-06,
      "loss": 0.6987,
      "step": 1210
    },
    {
      "epoch": 1.7110799438990183,
      "grad_norm": 1.5827082395553589,
      "learning_rate": 8.592800374006546e-06,
      "loss": 0.593,
      "step": 1220
    },
    {
      "epoch": 1.7251051893408134,
      "grad_norm": 4.83100700378418,
      "learning_rate": 8.49929873772791e-06,
      "loss": 0.7089,
      "step": 1230
    },
    {
      "epoch": 1.7391304347826086,
      "grad_norm": 1.7131577730178833,
      "learning_rate": 8.405797101449275e-06,
      "loss": 0.6412,
      "step": 1240
    },
    {
      "epoch": 1.753155680224404,
      "grad_norm": 3.4999468326568604,
      "learning_rate": 8.312295465170642e-06,
      "loss": 0.6231,
      "step": 1250
    },
    {
      "epoch": 1.767180925666199,
      "grad_norm": 4.065256118774414,
      "learning_rate": 8.218793828892006e-06,
      "loss": 0.5072,
      "step": 1260
    },
    {
      "epoch": 1.7812061711079945,
      "grad_norm": 1.9731215238571167,
      "learning_rate": 8.125292192613371e-06,
      "loss": 0.6123,
      "step": 1270
    },
    {
      "epoch": 1.7952314165497896,
      "grad_norm": 2.1217782497406006,
      "learning_rate": 8.031790556334737e-06,
      "loss": 0.6581,
      "step": 1280
    },
    {
      "epoch": 1.8092566619915849,
      "grad_norm": 1.775691032409668,
      "learning_rate": 7.938288920056102e-06,
      "loss": 0.6791,
      "step": 1290
    },
    {
      "epoch": 1.8232819074333801,
      "grad_norm": 2.6926369667053223,
      "learning_rate": 7.844787283777467e-06,
      "loss": 0.5669,
      "step": 1300
    },
    {
      "epoch": 1.8373071528751752,
      "grad_norm": 2.059208393096924,
      "learning_rate": 7.751285647498831e-06,
      "loss": 0.6117,
      "step": 1310
    },
    {
      "epoch": 1.8513323983169705,
      "grad_norm": 2.156085968017578,
      "learning_rate": 7.657784011220198e-06,
      "loss": 0.6146,
      "step": 1320
    },
    {
      "epoch": 1.8653576437587658,
      "grad_norm": 3.263256311416626,
      "learning_rate": 7.564282374941561e-06,
      "loss": 0.6086,
      "step": 1330
    },
    {
      "epoch": 1.8793828892005608,
      "grad_norm": 5.157161235809326,
      "learning_rate": 7.470780738662927e-06,
      "loss": 0.6115,
      "step": 1340
    },
    {
      "epoch": 1.8934081346423564,
      "grad_norm": 5.416324138641357,
      "learning_rate": 7.377279102384292e-06,
      "loss": 0.6409,
      "step": 1350
    },
    {
      "epoch": 1.9074333800841514,
      "grad_norm": 1.9079123735427856,
      "learning_rate": 7.283777466105657e-06,
      "loss": 0.6056,
      "step": 1360
    },
    {
      "epoch": 1.9214586255259467,
      "grad_norm": 2.9835352897644043,
      "learning_rate": 7.1902758298270225e-06,
      "loss": 0.5764,
      "step": 1370
    },
    {
      "epoch": 1.935483870967742,
      "grad_norm": 2.0095224380493164,
      "learning_rate": 7.096774193548388e-06,
      "loss": 0.6278,
      "step": 1380
    },
    {
      "epoch": 1.949509116409537,
      "grad_norm": 2.037811756134033,
      "learning_rate": 7.003272557269753e-06,
      "loss": 0.607,
      "step": 1390
    },
    {
      "epoch": 1.9635343618513323,
      "grad_norm": 5.722405433654785,
      "learning_rate": 6.909770920991118e-06,
      "loss": 0.6532,
      "step": 1400
    },
    {
      "epoch": 1.9775596072931276,
      "grad_norm": 1.7368301153182983,
      "learning_rate": 6.816269284712484e-06,
      "loss": 0.5318,
      "step": 1410
    },
    {
      "epoch": 1.9915848527349227,
      "grad_norm": 4.736629009246826,
      "learning_rate": 6.7227676484338475e-06,
      "loss": 0.7004,
      "step": 1420
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.6719242902208202,
      "eval_f1": 0.6341237811586576,
      "eval_loss": 0.6311476826667786,
      "eval_precision": 0.6728997938955054,
      "eval_recall": 0.6719242902208202,
      "eval_runtime": 22.9975,
      "eval_samples_per_second": 27.568,
      "eval_steps_per_second": 3.479,
      "step": 1426
    }
  ],
  "logging_steps": 10,
  "max_steps": 2139,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3040859005747200.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
